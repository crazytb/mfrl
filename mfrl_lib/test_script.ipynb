{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_STEPS = 300\n",
    "MAX_EPISODES = 200\n",
    "GAMMA = 0.98\n",
    "LEARNING_RATE = 0.0001\n",
    "N_OBSERVATIONS = 4\n",
    "N_ACTIONS = 2\n",
    "print_interval = 10\n",
    "ENERGY_COEFF = 1\n",
    "ENTROPY_COEFF = 0.01\n",
    "CRITIC_COEFF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "NODE_N = 5\n",
    "arrival_rate = np.linspace(0, 1, NODE_N+2).tolist()[1:-1]\n",
    "\n",
    "\n",
    "class MFRLFullEnv(gym.Env):\n",
    "    def __init__(self, agent):\n",
    "        super().__init__()\n",
    "        self.n = agent.topology.n\n",
    "        self.topology = agent.topology\n",
    "        self.arrival_rate = agent.arrival_rate\n",
    "        self.counter = 0\n",
    "        self.age = np.zeros(self.n)\n",
    "        self.max_aoi = np.zeros(self.n)\n",
    "        self.states = np.zeros((self.n, 3))  # States for each device: [idle, success, collision]\n",
    "        self.states[:, 0] = 1  # Initialize all devices to idle state\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"devices\": spaces.Tuple([\n",
    "                spaces.Dict({\n",
    "                    \"state\": spaces.MultiBinary(3),  # One-hot encoded {idle:0, success:1, collision:2}\n",
    "                    \"age\": spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "                }) for _ in range(self.n)\n",
    "            ]),\n",
    "            \"counter\": spaces.Discrete(MAX_STEPS)\n",
    "        })\n",
    "\n",
    "        # Define action space as MultiBinary for direct binary actions\n",
    "        self.action_space = spaces.MultiBinary(self.n)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.counter = 0\n",
    "        self.age = np.zeros(self.n)\n",
    "        self.max_aoi = np.zeros(self.n)\n",
    "        # Reset all devices to idle state\n",
    "        self.states = np.zeros((self.n, 3))\n",
    "        self.states[:, 0] = 1\n",
    "        \n",
    "        observation = {\n",
    "            \"devices\": tuple({\n",
    "                \"state\": self.states[i],\n",
    "                \"age\": np.array([self.age[i]], dtype=np.float32)\n",
    "            } for i in range(self.n)),\n",
    "            \"counter\": self.counter\n",
    "        }\n",
    "\n",
    "        info = {}\n",
    "        return observation, info\n",
    "    \n",
    "    def set_all_actions(self, actions):\n",
    "        self.all_actions = np.array(actions)\n",
    "        \n",
    "    def get_maxaoi(self):\n",
    "        return self.max_aoi\n",
    "    \n",
    "    def set_max_aoi(self, max_aoi):\n",
    "        self.max_aoi_set = max_aoi\n",
    "        \n",
    "    def idle_check(self):\n",
    "        return all(self.all_actions[self.adj_ids] == 0)\n",
    "        \n",
    "    def get_adjacent_nodes(self, *args):\n",
    "        if len(args) > 0:\n",
    "            return np.where(self.topology.adjacency_matrix[args[0]] == 1)[0]\n",
    "        else:\n",
    "            return np.where(self.topology.adjacency_matrix[self.id] == 1)[0]\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        self.age += 1 / MAX_STEPS\n",
    "        \n",
    "        # Track which devices attempt transmission\n",
    "        transmitting_devices = []\n",
    "        energy_reward = 0  # Only track energy reward during steps\n",
    "        \n",
    "        # First pass: identify all transmitting devices\n",
    "        for ind, act in enumerate(action):\n",
    "            if act == 1 and self.arrival_rate[ind] > np.random.rand():\n",
    "                transmitting_devices.append(ind)\n",
    "                energy_reward -= ENERGY_COEFF  # Energy cost for transmission attempt\n",
    "        \n",
    "        # Second pass: determine transmission outcomes and update states\n",
    "        new_states = np.zeros((self.n, 3))\n",
    "        \n",
    "        for ind in range(self.n):\n",
    "            if ind in transmitting_devices:\n",
    "                # Check if this is the only device transmitting\n",
    "                if len(transmitting_devices) == 1:\n",
    "                    # Successful transmission\n",
    "                    new_states[ind, 1] = 1  # Success state\n",
    "                    self.age[ind] = 0  # Reset age\n",
    "                else:\n",
    "                    # Collision occurred\n",
    "                    new_states[ind, 2] = 1  # Collision state\n",
    "            else:\n",
    "                # Device remained idle\n",
    "                new_states[ind, 0] = 1  # Idle state\n",
    "        \n",
    "        self.states = new_states\n",
    "        self.max_aoi = np.maximum(self.age, self.max_aoi)\n",
    "        \n",
    "        # Construct new observation\n",
    "        observation = {\n",
    "            \"devices\": tuple({\n",
    "                \"state\": self.states[i],\n",
    "                \"age\": np.array([self.age[i]], dtype=np.float32)\n",
    "            } for i in range(self.n)),\n",
    "            \"counter\": self.counter\n",
    "        }\n",
    "\n",
    "        # Check termination condition\n",
    "        terminated = self.counter == MAX_STEPS\n",
    "        \n",
    "        # Calculate final reward\n",
    "        if terminated:\n",
    "            # Calculate average AoI reward across all devices at episode end\n",
    "            aoi_reward = np.mean((1 - self.max_aoi)) * MAX_STEPS\n",
    "            total_reward = energy_reward/self.n + aoi_reward\n",
    "        else:\n",
    "            # During episode, only return energy reward\n",
    "            total_reward = energy_reward/self.n\n",
    "            \n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        return observation, total_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Initial Observation]\n",
      "{'devices': ({'state': array([1., 0., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.], dtype=float32)}), 'counter': 0}\n",
      "\n",
      "[Step 0]\n",
      "Action Taken: [0 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.1], dtype=float32)}, {'state': array([0., 1., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.1], dtype=float32)}), 'counter': 1}\n",
      "Reward: -0.03333333333333333\n",
      "Terminated: False\n",
      "\n",
      "[Step 1]\n",
      "Action Taken: [0 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.2], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.1], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.2], dtype=float32)}), 'counter': 2}\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "\n",
      "[Step 2]\n",
      "Action Taken: [0 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.3], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.2], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.3], dtype=float32)}), 'counter': 3}\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "\n",
      "[Step 3]\n",
      "Action Taken: [0 1 1], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.4], dtype=float32)}, {'state': array([0., 0., 1.]), 'age': array([0.3], dtype=float32)}, {'state': array([0., 0., 1.]), 'age': array([0.4], dtype=float32)}), 'counter': 4}\n",
      "Reward: -0.06666666666666667\n",
      "Terminated: False\n",
      "\n",
      "[Step 4]\n",
      "Action Taken: [0 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.5], dtype=float32)}, {'state': array([0., 1., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.5], dtype=float32)}), 'counter': 5}\n",
      "Reward: -0.03333333333333333\n",
      "Terminated: False\n",
      "\n",
      "[Step 5]\n",
      "Action Taken: [1 1 1], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([0., 1., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.1], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.6], dtype=float32)}), 'counter': 6}\n",
      "Reward: -0.03333333333333333\n",
      "Terminated: False\n",
      "\n",
      "[Step 6]\n",
      "Action Taken: [0 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.1], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.2], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.7], dtype=float32)}), 'counter': 7}\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "\n",
      "[Step 7]\n",
      "Action Taken: [1 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.2], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.3], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.8], dtype=float32)}), 'counter': 8}\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "\n",
      "[Step 8]\n",
      "Action Taken: [0 0 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.3], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.4], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([0.9], dtype=float32)}), 'counter': 9}\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "\n",
      "[Step 9]\n",
      "Action Taken: [1 1 0], Arrival Rates: [0.41302092 0.71561128 0.77463762]\n",
      "Observation: {'devices': ({'state': array([1., 0., 0.]), 'age': array([0.4], dtype=float32)}, {'state': array([0., 1., 0.]), 'age': array([0.], dtype=float32)}, {'state': array([1., 0., 0.]), 'age': array([1.], dtype=float32)}), 'counter': 10}\n",
      "Reward: 3.6333333333333337\n",
      "Terminated: True\n",
      "\n",
      "[Episode Finished]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a dummy agent with a topology\n",
    "class DummyAgent:\n",
    "    class Topology:\n",
    "        def __init__(self, n):\n",
    "            self.n = n\n",
    "            self.adjacency_matrix = np.eye(n)  # Example: No connections (identity matrix)\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.topology = self.Topology(n)\n",
    "        self.arrival_rate = np.random.uniform(0.2, 0.8, size=n)  # Random arrival rates for each device\n",
    "\n",
    "# Define MAX_STEPS and ENERGY_COEFF\n",
    "MAX_STEPS = 10  # Example: limit steps per episode\n",
    "ENERGY_COEFF = 0.1  # Example: energy penalty\n",
    "\n",
    "# Create environment\n",
    "n_devices = 3  # Example: 3 devices\n",
    "dummy_agent = DummyAgent(n_devices)\n",
    "env = MFRLFullEnv(dummy_agent)\n",
    "\n",
    "# Reset environment\n",
    "obs, info = env.reset()\n",
    "print(\"\\n[Initial Observation]\")\n",
    "print(obs)\n",
    "\n",
    "# Step through the environment\n",
    "done = False\n",
    "step_count = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.random.randint(2, size=n_devices)  # Choose a random action\n",
    "    obs, reward, done, _, info = env.step(action)  # Take a step\n",
    "\n",
    "    print(f\"\\n[Step {step_count}]\")\n",
    "    print(f\"Action Taken: {action}, Arrival Rates: {dummy_agent.arrival_rate}\")\n",
    "    print(f\"Observation: {obs}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Terminated: {done}\")\n",
    "\n",
    "    step_count += 1\n",
    "\n",
    "print(\"\\n[Episode Finished]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
